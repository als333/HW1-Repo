{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LING1340 Homework 1\n",
    "Name: **Daniel Zheng**\n",
    "\n",
    "Email: **daniel.zheng@pitt.edu**\n",
    "\n",
    "Due Date: **September 5, 2017**\n",
    "\n",
    "#### Dataset\n",
    "Large Movie Review Dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data\n",
    "def load(filepath):\n",
    "    files = glob.glob(filepath)\n",
    "    raw = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            raw.append(f.read())\n",
    "    return raw\n",
    "train_neg_raw = load('data/aclImdb/train/neg/*')\n",
    "train_pos_raw = load('data/aclImdb/train/pos/*')\n",
    "test_neg_raw = load('data/aclImdb/test/neg/*')\n",
    "test_pos_raw = load('data/aclImdb/test/pos/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Wimpy stuffed shirt Armand Louque (blandly played by veteran character actor Dean Jagger in a rare lead role) joins a group of researchers who want to find and destroy the secret technique of creating zombies. Armand falls for the lovely Claire Duval (fetching blonde Dorothy Stone), who uses the meek sap to get Armand's colleague Clifford Grayson (the hopelessly wooden Robert Noland) to marry her. Furious over being used and spurned by Claire, Armand uses his knowledge of voodoo to get revenge. Sound exciting? Well, it sure ain't. For starters, Victor Halperin's static (non)direction lets the meandering and uneventful talk-ridden story plod along at an excruciatingly slow pace. Worse yet, Halperin crucially fails to bring any tension, atmosphere and momentum to the hideously tedious proceedings. The mostly blah acting from a largely insipid cast doesn't help matters any; only George Cleveland as the hearty General Duval and E. Alyn Warren as the irascible Dr. Trevissant manage to enliven things a bit with their welcome and refreshing hammy histrionics. The drippy stock film library score, the painfully obvious stagebound sets, and the crude cinematography are pretty lousy and unimpressive as well. In fact, this feeble excuse for a fright feature is so crummy that not even the uncredited starkly staring eyes of the great Bela Lugosi can alleviate the brain-numbing boredom. A dismally dull dud.\", \"One of the other commenters mentioned that they almost walked out. If I hadn't been with my wife, who wanted to stay, I would have left. It's a shame, too, because I think it could have been a good movie. But this is easily one of the worst adapted screenplays I've ever seen. It starts out nowhere and it goes nowhere (I would say it goes nowhere fast, but it really goes nowhere slow...painfully slow). From time to time there are hints that something interesting might happen, or that there is potentially some depth underneath one of the characters, but that's all we get - hints. There is not a single payoff or revelation in the entire movie. Not that I need a slick plot to be entertained...I love a good meandering character study as much as the next indie buff. But these characters add up to nothing. For the entire duration of the film you don't care what happens to a single one of them. As a matter of fact, you almost start hoping they die, because at least a death might be more interesting than watching their inexplicable behavior, which is so strange and unpredictable that you'd think it in itself would be compelling, but it's not. Instead of quirky, noir-esquire characters acting in hard-boiled fashion, you simply recognize it immediately for what it is: a bunch of talented but miscast actors, brooding and raising their eyebrows while reading bizarre dialogue without a hint of relevant context. All this for two plodding, painfully slow hours. Awful.\"]\n"
     ]
    }
   ],
   "source": [
    "print(train_neg_raw[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"All Kira Reed fans MUST see this. The film's premise has struggling romance novelist Kira unable to come up with any new ideas. She's also getting over a divorce. However, she meets this guy at a restaurant and he helps her out of her shell (and clothing). They go into a corner room and they do it. Thankfully, Kira gets a condom out (Now don't ever tell me these Playboy films are worthless piles of soft-core fluff. Remember kids, safe sex). Later, she marvels to her publishist how great it was, but she didn't get his name. Despite this, the guy finds her and they continue their kinky games. But eventually she tires of his sneakiness and wants to know more. When she does, all hell breaks loose, and I'll leave it at that. This is easily the best of these soft-core Playboys films I've seen. Check this out, and marvel at the greatness of Kira.\", 'I was not expecting much from this movie, but was very pleasantly surprised, as it is light and funny and very well observed. The central trio of deadbeat bikers were surprisingly likable as they staggered and clowned their way through their drug-centred trip to Wales. The humour was gentle and subtle, as indeed were the three characters (witness their sympathetic treatment of the little old lady shopkeeper). The atmospherics of rural Wales were captured perfectly, and the soundtrack was very well chosen. Cleverly and carefully scripted, with great attention to detail - I have never seen such a realistic portrayal of alternative culture - I felt I was there with them. Very light in touch and full of fun - not what you might expect from a movie about bikers and drugs. A delight on all fronts, and difficult to criticise, though I thought the last two scenes were a bit lame - the film should have ended when they left Wales. But overall, an unexpected treasure of a film.']\n"
     ]
    }
   ],
   "source": [
    "print(train_pos_raw[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "There are train and test sets of data. Within `train` and `test`, there is a `neg` and `pos` folder each with 12,500 negative and positive samples. In the `train` folder there is also a folder called `unsup` with 50,000 examples for unsupervised learning.\n",
    "\n",
    "### Processing\n",
    "A lot of the data contains `<br>` tags from HTML, which will have to be cleaned up. I will weight using term frequency - inverse document frequency (tf-idf) and train a naive bayes classifier with sci-kit learn. For classifying movie reviews, I have a feeling adjectives and verbs are the most important, because words like \"terrible\", \"amazing\", \"thrilled\", and \"enjoyed\", just intuitively seem like they are more indicative of sentiment. For now, I will only keep those parts of speech, but it is definitely worth investigating using the other parts of speech.\n",
    "\n",
    "### Expected problems with approach\n",
    "A lot of the movie review data is going to be background information on the movies that probably won't be helpful for learning sentiment. Even though tf-idf will deemphasize many common words like \"is\", it might emphasize rare background info even more than common reviewing terms like \"terrible\", \"fantastic\", etc... Since background info terms should have low-frequency, it shouldn't make a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function clean up data, taking out punctuation, numbers and special characters\n",
    "def clean(raw_input):\n",
    "    # tokenize and remove invalid characters\n",
    "    tokens = [word_tokenize(' '.join([x for x in string.split() if re.sub('[a-zA-Z0-9_.,!\"\\'-/]', '', x) == ''])) for string in raw_input]\n",
    "    # part of speech tags\n",
    "    review_pos = [pos_tag(x) for x in tokens]\n",
    "    # keep only adjectives and verbs for every review\n",
    "    cleaned = [' '.join([word for (word,pos) in phrase_pos if pos.find('JJ') != -1 or pos.find('V') != -1]) for phrase_pos in review_pos]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a while because tokenizing + pos_tag is slow\n",
    "train_neg = clean(train_neg_raw)\n",
    "train_pos = clean(train_pos_raw)\n",
    "test_neg = clean(test_neg_raw)\n",
    "test_pos = clean(test_pos_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuffed shirt played veteran rare lead joins want find destroy secret creating falls lovely fetching uses meek get wooden marry Furious being used spurned uses get sure ai static lets uneventful talk-ridden slow Worse fails bring tedious blah acting insipid does help irascible enliven refreshing hammy drippy library obvious are pretty lousy unimpressive feeble fright is crummy uncredited staring great alleviate brain-numbing dull', \"other mentioned walked had been wanted stay have left 's think have been good is worst adapted 've seen starts goes say goes goes slow slow are interesting happen is depth underneath 's get is single entire need be entertained love good next add entire do care happens single start hoping die least be interesting watching inexplicable is strange unpredictable think be compelling 's quirky noir-esquire acting hard-boiled recognize talented miscast raising reading bizarre relevant slow\"]\n",
      "total negative training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "# example of what the filtered data looks like\n",
    "print(train_neg[:2])\n",
    "print('total negative training samples:', len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"see has struggling unable come new 's getting meets helps go do gets do tell are worthless soft-core safe marvels publishist great was did get finds continue tires wants know more does leave is best soft-core 've seen Check\", 'was expecting was surprised is light funny observed central were likable staggered clowned drug-centred was gentle subtle were witness sympathetic little old rural were captured was chosen scripted great detail have seen realistic alternative felt was light full expect difficult criticise thought last were lame have ended left overall unexpected']\n",
      "total positive training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[:2])\n",
    "print('total positive training samples:', len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some visualizations\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for phrase in train_neg:\n",
    "    word_counts.update(phrase.strip('.,?!\"\\'').lower() for word in phrase.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating labels\n",
    "Train and test labels are assigned using `0` as the negative class and `1` as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_neg + train_pos # concatenate for vectorizing\n",
    "train_labels = [0]*len(train_neg) + [1]*len(train_pos) # labels\n",
    "test = test_neg + test_pos\n",
    "test_labels = [0]*len(test_neg) + [1]*len(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# should be the same as CountVectorizer combined with TfidfTransformer\n",
    "tfidf = TfidfVectorizer()\n",
    "train_vectors = tfidf.fit_transform(train)\n",
    "# already fit to training set, so just transform\n",
    "test_vectors = tfidf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 30195)\n",
      "(25000, 30195)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(train_vectors, train_labels)\n",
    "print(np.mean(classifier.predict(train_vectors) == train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.308 % accuracy\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(test_vectors)\n",
    "print(np.mean(predicted == test_labels)*100, '% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "So 84.3% accuracy is pretty good! Just for fun, I put together my own test set of movie review strings to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test = [\"This movie was the worst. I hate it.\", \"Terrible acting. Negative, bland, uninteresting.\", \n",
    "               \"This movie was great, I really enjoyed the acting!\", \n",
    "               \"Amazing storyline, hilarious characters, and a shocking ending.\", \n",
    "               \"The vague plot was ridiculously boring, and put me to sleep.\"]\n",
    "custom_labels = [0,0,1,1,0]\n",
    "custom_test_vectors = tfidf.transform(clean(custom_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "custom_predictions = classifier.predict(custom_test_vectors)\n",
    "print(np.mean(custom_predictions == custom_labels))\n",
    "print(custom_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This code does a few things:\n",
    "1. Reads in movie review data so that each review is one string in a list\n",
    "2. Preprocesses, removing everything but adjectives and verbs within each review.\n",
    "3. Creates train and test tf-idf vectors\n",
    "4. Fits a naive-bayes classifier to the train vector\n",
    "5. Test on testing data\n",
    "So it looks like using tf-idf with a Multinomial Naive-Bayes classifier can pretty reliably guess binary sentiment of a movie review. This was by no means a comprehensive study, however. \n",
    "\n",
    "I would have liked to try other classifiers, and also experiment with some of the TfidfVectorizer parameters. Also, I would like to understand tf-idf and naive-bayes in a more in-depth manner, beyond just how to use them in code. \n",
    "\n",
    "Perhaps another interesting project to do with this data would be to generate positive and negative movie reviews, or to use autoencoders to do some unsupervised learning since they conveniently provided a folder of 50,000 unlabeled movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
