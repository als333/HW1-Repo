{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LING1340 Homework 1\n",
    "Name: **Daniel Zheng**\n",
    "\n",
    "Email: **daniel.zheng@pitt.edu**\n",
    "\n",
    "Due Date: **September 5, 2017**\n",
    "\n",
    "#### Dataset\n",
    "Large Movie Review Dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re, glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading training data\n",
    "def load(filepath):\n",
    "    files = glob.glob(filepath)\n",
    "    raw = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            raw.append(f.read())\n",
    "    return raw\n",
    "train_neg_raw = load('data/aclImdb/train/neg/*')\n",
    "train_pos_raw = load('data/aclImdb/train/pos/*')\n",
    "test_neg_raw = load('data/aclImdb/test/neg/*')\n",
    "test_pos_raw = load('data/aclImdb/test/pos/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative training sample: [ \"Man, this movie sucked big time! I didn't even manage to see the hole thing (my girlfriend did though). Really bad acting, computer animations so bad you just laugh (woman to werewolf), strange clips, the list goes on and on. Don't know if its just me or does this movie remind you of a porn movie? And I don't mean all the naked ladys... It's something about the light or something... This could maybee become a classic just because of the bad acting and all the naked women, but not because it's an original movie white a nice plot twist. My final words are: Don't see it! It's not worth the time. If you wanna see it because the nakedness there's lots of better ones to see!\"]\n"
     ]
    }
   ],
   "source": [
    "print('Negative training sample:', np.random.choice(train_neg_raw, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive training sample:  [ \"This is an astounding film. As well as showing actual footage of key events in the failed coup to oust Chavez, we are given the background picture which describes a class-divided society. Many of the rich, it appears, have a choice with the people's democratic choice, and are willing to use the military for regime change. 'Be careful what you say in front of your servants' is a revealing comment. The head of the country's biggest oil company appoints himself as the new president, with US backing, and these young Irish film makers have it all on camera. A great film to educate young people about democracy. We see transparent documentation of how media can be manipulated, and force used, in the interests of big business, against the interests of the democratic wishes of the people. Riveting stuff.\"]\n"
     ]
    }
   ],
   "source": [
    "print('Positive training sample: ', np.random.choice(train_pos_raw, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "There are train and test sets of data. Within `train` and `test`, there is a `neg` and `pos` folder each with 12,500 negative and positive samples. In the `train` folder there is also a folder called `unsup` with 50,000 examples for unsupervised learning.\n",
    "\n",
    "### Processing\n",
    "A lot of the data contains `<br>` tags from HTML, which will have to be cleaned up. I will weight using term frequency - inverse document frequency (tf-idf) and train a naive bayes classifier with sci-kit learn. For classifying movie reviews, I have a feeling adjectives and verbs are the most important, because words like \"terrible\", \"amazing\", \"thrilled\", and \"enjoyed\" just intuitively seem like they are more indicative of sentiment. For now, I will only keep those parts of speech, but it is definitely worth investigating using the other parts of speech.\n",
    "\n",
    "### Expected problems with approach\n",
    "A lot of the movie review data is going to be background information on the movies that probably won't be helpful for learning sentiment. Even though tf-idf will deemphasize many common words like \"is\", it might emphasize rare background info even more than common reviewing terms like \"terrible\", \"fantastic\", etc... Since background info terms should have low-frequency, it shouldn't make a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function clean up data, taking out punctuation, numbers and special characters\n",
    "def clean(raw_input):\n",
    "    # tokenize and remove invalid characters\n",
    "    tokens = [word_tokenize(' '.join([x for x in string.split() if re.sub('[a-zA-Z0-9_.,!\"\\'-/]', '', x) == ''])) for string in raw_input]\n",
    "    # part of speech tags\n",
    "    review_pos = [pos_tag(x) for x in tokens]\n",
    "    # keep only adjectives and verbs for every review\n",
    "    cleaned = [' '.join([word for (word,pos) in phrase_pos if pos.find('JJ') != -1 or pos.find('V') != -1]) for phrase_pos in review_pos]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a while because tokenizing + pos_tag is slow\n",
    "train_neg = clean(train_neg_raw)\n",
    "train_pos = clean(train_pos_raw)\n",
    "test_neg = clean(test_neg_raw)\n",
    "test_pos = clean(test_pos_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuffed shirt played veteran rare lead joins want find destroy secret creating falls lovely fetching uses meek get wooden marry Furious being used spurned uses get sure ai static lets uneventful talk-ridden slow Worse fails bring tedious blah acting insipid does help irascible enliven refreshing hammy drippy library obvious are pretty lousy unimpressive feeble fright is crummy uncredited staring great alleviate brain-numbing dull', \"other mentioned walked had been wanted stay have left 's think have been good is worst adapted 've seen starts goes say goes goes slow slow are interesting happen is depth underneath 's get is single entire need be entertained love good next add entire do care happens single start hoping die least be interesting watching inexplicable is strange unpredictable think be compelling 's quirky noir-esquire acting hard-boiled recognize talented miscast raising reading bizarre relevant slow\"]\n",
      "total negative training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "# example of what the filtered data looks like\n",
    "print(train_neg[:2])\n",
    "print('total negative training samples:', len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"see has struggling unable come new 's getting meets helps go do gets do tell are worthless soft-core safe marvels publishist great was did get finds continue tires wants know more does leave is best soft-core 've seen Check\", 'was expecting was surprised is light funny observed central were likable staggered clowned drug-centred was gentle subtle were witness sympathetic little old rural were captured was chosen scripted great detail have seen realistic alternative felt was light full expect difficult criticise thought last were lame have ended left overall unexpected']\n",
      "total positive training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[:2])\n",
    "print('total positive training samples:', len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some visualizations\n",
    "from collections import Counter\n",
    "neg_word_counts = Counter()\n",
    "pos_word_counts = Counter()\n",
    "for neg, pos in zip(train_neg, train_pos):\n",
    "    neg_word_counts.update(word.strip('.,?!\"\\'').lower() for word in neg.split())\n",
    "    pos_word_counts.update(word.strip('.,?!\"\\'').lower() for word in pos.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common negative words: [('is', 51501), ('was', 27479), ('have', 15414), ('s', 15220), ('are', 15001), ('be', 14262), ('do', 9823), ('has', 7690), ('good', 7014), ('bad', 6696), ('were', 6258), ('had', 6237), ('did', 5831), ('does', 5354), ('see', 5278), ('been', 5056), ('get', 4814), ('make', 4447), ('made', 4284), ('other', 4211), ('think', 3489), ('being', 3339), ('more', 3237), ('first', 3211), ('know', 3205), ('seen', 3166), ('watch', 3072), ('much', 2999), ('say', 2859), ('many', 2825), ('ve', 2748), ('m', 2705), ('little', 2667), ('go', 2481), ('great', 2466), ('watching', 2452), ('going', 2259), ('re', 2205), ('worst', 2199), ('few', 2134), ('want', 2102), ('better', 2080), ('such', 2056), ('same', 2018), ('real', 2003), ('seems', 2000), ('got', 1980), ('least', 1972), ('most', 1957), ('only', 1953), ('funny', 1929), ('old', 1892), ('original', 1882), ('find', 1831), ('give', 1782), ('makes', 1780), ('best', 1745), ('gets', 1692), ('take', 1658), ('whole', 1577), ('thought', 1540), ('interesting', 1500), ('acting', 1490), ('am', 1476), ('trying', 1469), ('like', 1454), ('come', 1433), ('done', 1431), ('big', 1423), ('look', 1421), ('believe', 1390), ('saw', 1366), ('looks', 1347), ('last', 1341), ('poor', 1338), ('new', 1330), ('awful', 1330), ('making', 1323), ('terrible', 1302), ('own', 1292), ('stupid', 1272), ('looking', 1264), ('found', 1250), ('having', 1222), ('sure', 1219), ('put', 1219), ('goes', 1219), ('comes', 1209), ('main', 1200), ('said', 1184), ('supposed', 1178), ('young', 1153), ('seem', 1134), ('watched', 1131), ('boring', 1117), ('ending', 1090), ('let', 1090), ('worse', 1062), ('feel', 1055), ('i', 1053)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common negative words:',neg_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common positive words: [('is', 58219), ('was', 22565), ('are', 15035), ('s', 13437), ('have', 12948), ('be', 12187), ('has', 9336), ('good', 7245), ('do', 6926), ('great', 5885), ('see', 5712), ('had', 5222), ('does', 4909), ('were', 4897), ('other', 4737), ('did', 4305), ('been', 4157), ('get', 4115), ('best', 3683), ('many', 3663), ('more', 3637), ('made', 3610), ('think', 3513), ('first', 3512), ('seen', 3317), ('being', 3164), ('make', 3104), ('little', 2969), ('watch', 2912), ('know', 2731), ('much', 2660), ('real', 2369), ('makes', 2362), ('say', 2331), ('most', 2253), ('ve', 2234), ('find', 2220), ('go', 2200), ('such', 2156), ('young', 2091), ('own', 1966), ('love', 1943), ('old', 1932), ('same', 1923), ('few', 1891), ('watching', 1815), ('m', 1787), ('bad', 1739), ('going', 1729), ('funny', 1680), ('new', 1670), ('got', 1639), ('saw', 1599), ('seems', 1582), ('like', 1561), ('played', 1548), ('take', 1541), ('excellent', 1532), ('give', 1517), ('want', 1513), ('big', 1499), ('re', 1495), ('last', 1494), ('gets', 1484), ('better', 1481), ('done', 1462), ('different', 1457), ('come', 1453), ('beautiful', 1392), ('interesting', 1391), ('thought', 1377), ('true', 1339), ('original', 1321), ('found', 1283), ('wonderful', 1263), ('am', 1251), ('comes', 1247), ('plays', 1238), ('takes', 1227), ('having', 1219), ('only', 1201), ('feel', 1173), ('goes', 1159), ('whole', 1139), ('ending', 1092), ('least', 1079), ('watched', 1078), ('sure', 1073), ('put', 1067), ('seeing', 1060), ('perfect', 1052), ('loved', 1052), ('look', 1041), ('nice', 1040), ('believe', 1036), ('i', 1031), ('gives', 1030), ('enjoy', 1025), ('acting', 1021), ('seem', 1015)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common positive words:',pos_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counts\n",
    "Looking at the processed data above, words like \"is\" and \"was\" are very common, as expected. The negative set also has negative words like \"awful\", \"terrible\", and \"stupid\", while the positive set has words like \"perfect\", \"excellent\", and \"beautiful\". This is a good sign! After applying tf-idf, it should be pretty easy for a classifier to determine sentiment.\n",
    "\n",
    "### Creating labels\n",
    "Train and test labels are assigned using `0` as the negative class and `1` as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_neg + train_pos # concatenate for vectorizing\n",
    "train_labels = [0]*len(train_neg) + [1]*len(train_pos) # labels\n",
    "test = test_neg + test_pos\n",
    "test_labels = [0]*len(test_neg) + [1]*len(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# should be the same as CountVectorizer combined with TfidfTransformer\n",
    "tfidf = TfidfVectorizer()\n",
    "train_vectors = tfidf.fit_transform(train)\n",
    "# already fit to training set, so just transform\n",
    "test_vectors = tfidf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 30197)\n",
      "(25000, 30197)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(train_vectors, train_labels)\n",
    "print(np.mean(classifier.predict(train_vectors) == train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.308 % accuracy\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(test_vectors)\n",
    "print(np.mean(predicted == test_labels)*100, '% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "84.3% accuracy is pretty good! Definitely better than expected. Just for fun, I put together my own test set of movie review strings to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_test = [\"This movie was the worst. I hate it.\", \"Terrible acting. Negative, bland, uninteresting.\", \n",
    "               \"This movie was great, I really enjoyed the acting!\", \n",
    "               \"Amazing storyline, hilarious characters, and a shocking ending.\", \n",
    "               \"The vague plot was ridiculously boring, and put me to sleep.\"]\n",
    "custom_labels = [0,0,1,1,0]\n",
    "custom_test_vectors = tfidf.transform(clean(custom_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % accuracy on custom test set\n",
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "custom_predictions = classifier.predict(custom_test_vectors)\n",
    "print(np.mean(custom_predictions == custom_labels)*100, '% accuracy on custom test set')\n",
    "print(custom_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This code does a few things:\n",
    "1. Reads in movie review data so that each review is one string in a list\n",
    "2. Preprocesses, removing everything but adjectives and verbs within each review.\n",
    "3. Creates train and test tf-idf vectors\n",
    "4. Fits a naive-bayes classifier to the train vector\n",
    "5. Test on testing data\n",
    "So it looks like using tf-idf with a Multinomial Naive-Bayes classifier can pretty reliably guess binary sentiment of a movie review. This was by no means a comprehensive study, however. \n",
    "\n",
    "I would have liked to try other classifiers, and also experiment with some of the TfidfVectorizer parameters. Also, I would like to understand tf-idf and naive-bayes in a more in-depth manner, beyond just how to use them in code. \n",
    "\n",
    "Perhaps another interesting project to do with this data would be to generate positive and negative movie reviews, or to use autoencoders to do some unsupervised learning since they conveniently provided a folder of 50,000 unlabeled movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
