{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Information\n",
    "\n",
    "Paige Haring\n",
    "\n",
    "peh40@pitt.edu\n",
    "\n",
    "September 1, 2017\n",
    "\n",
    "**Name:** Genesis Corpus\n",
    "\n",
    "**Authors:** N/A\n",
    "\n",
    "**Download URL:** http://www.nltk.org/nltk_data/\n",
    "\n",
    "**Makeup:** This corpus consists of 8 text files of written data, specifically, each file is a translation of the Book of Genesis. The 8 translations include: English- King James Bible, English- World English Bible, Finnish, French, German, lolcat, Portuguese, and Swedish. \n",
    "\n",
    "**License:** Public Domain\n",
    "\n",
    "**Noteworthy:** Of these translations, all represent a spoken human language except for lolcat, which is essentially a meme language \"spoken\" by cats on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Assessment\n",
    "**Summary:**\n",
    "\n",
    "**Future Wish:** I'd like to learn how to align parallel data automatically to use for translation. That might not even be possible with this corpus despite it all being translations of Genesis, because each text file has a different number of sentences and words. I believe this is because not only are these bibles written by different authors in different languages, but they were also all written in different time periods. I would like to learn more about what makes parallel data \"good\" parallel data in general and if it would be possible to align this or make use of this corpus in some other way to aid in translation. I'd also like to learn what the correct way to handle my encoding problem is. If you have a dataset with a bunch of wacky characters, and utf-8 can't handle it, what is the best way to proceed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 files in the corpus!\n",
      "english-kjv.txt has 44764 words, 1467 sents\n",
      "english-web.txt has 44054 words, 2232 sents\n",
      "finnish.txt has 32520 words, 2160 sents\n",
      "french.txt has 46116 words, 2004 sents\n",
      "german.txt has 43941 words, 1900 sents\n",
      "lolcat.txt has 17074 words, 822 sents\n",
      "portuguese.txt has 45094 words, 1669 sents\n",
      "swedish.txt has 41705 words, 1386 sents\n",
      "There is a total of 315268 words and 13640 sents in the entire corpus.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = 'data/genesis'\n",
    "\n",
    "#The encoding for each of the text files was different! This was very annoying and I had\n",
    "#a hard time finding which file used what. I eventually found it in the code on this page!\n",
    "#http://www.nltk.org/_modules/nltk/corpus.html\n",
    "\n",
    "gens = PlaintextCorpusReader(corpus_root, r'.*\\.txt', encoding=[\n",
    "        ('finnish|french|german', 'latin_1'),\n",
    "        ('swedish', 'cp865'),\n",
    "        ('.*', 'utf_8')])\n",
    "\n",
    "print('There are',len(gens.fileids()),'files in the corpus!')\n",
    "\n",
    "for f in gens.fileids():\n",
    "    print(f, 'has', len(gens.words(f)), 'words,', len(gens.sents(f)), 'sents')\n",
    "\n",
    "print('There is a total of', len(gens.words()), 'words and', len(gens.sents()), 'sents in the entire corpus.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery Question\n",
    "What are the five most frequent words in each text, and how much do these lists differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-kjv.txt : [('unto', 590), ('said', 476), ('thou', 272), ('thy', 267), ('thee', 257)]\n",
      "english-web.txt : [('said', 470), ('father', 270), ('God', 229), ('land', 195), ('Jacob', 185)]\n",
      "finnish.txt : [('sanoi', 324), ('Jumala', 180), ('Jaakob', 138), ('Herra', 123), ('Joosef', 114)]\n",
      "french.txt : [('fils', 317), ('Dieu', 230), ('Jacob', 202), ('père', 199), ('pays', 182)]\n",
      "german.txt : [('szlig', 677), ('sprach', 416), ('Gott', 207), ('h3', 200), ('Jakob', 159)]\n",
      "lolcat.txt : [('teh', 508), ('2', 189), ('wuz', 171), ('Ceiling', 166), ('Cat', 154)]\n",
      "portuguese.txt : [('terra', 349), ('Deus', 240), ('filhos', 209), ('pai', 200), ('Jacó', 180)]\n",
      "swedish.txt : [('sade', 400), ('skall', 283), ('Gud', 211), ('Jakob', 156), ('fader', 139)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#I want the five most frequent words excluding stopwords, so I'll bring in the stopwords corpus to be able to search for them.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for f in gens.fileids():\n",
    "    fd = nltk.FreqDist(gens.words(f))\n",
    "    common = fd.most_common(55)\n",
    "    stops = set(stopwords.words())\n",
    "    common = [(x,y) for (x,y) in common if x.isalnum() and x.lower() not in stops]\n",
    "    print(f, ':', common[:5])\n",
    " \n",
    "#\n",
    "#Clearly threre are a lot of problems with encoding = latin-1. \n",
    "#The characters aren't mapped the the correct symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a different attempt to read in the corpus where I'm manually choosing the encoding. This seems like a really silly way to do it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/genesis/*.txt')\n",
    "files_trimmed = [f[13:] for f in files]\n",
    "\n",
    "raw = []\n",
    "for file in files:\n",
    "    if encoding == 'utf-8':\n",
    "        with open(file, encoding = 'utf-8') as f:\n",
    "            txt = f.read()\n",
    "            raw.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
